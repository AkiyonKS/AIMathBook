{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AkiyonKS/AIMathBook/blob/master/notebooks/pca_and_cluster_analysis__group_name.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGDnKiJEqphs"
      },
      "source": [
        "# クラスター分析と主成分分析\n",
        "\n",
        "階層型クラスター分析と主成分分析を行うためのノートブックです。\n",
        "\n",
        "クラスター分析の結果を用いて主成分スコアの色分けをします。\n",
        "\n",
        "本ワークブックではグループの名前(group_name)を指定することで、バッチ処理が可能です。\n",
        "\n",
        "（主成分分析はまだ未完成）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6FFPFUAgxsF"
      },
      "source": [
        "##Google Driveに接続\n",
        "\n",
        "毎回最初に1回実行する。認証のページが開くので手続きする。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPY9W-ZgHu4Q",
        "outputId": "5990aa03-bc7f-4875-b249-31e7c6f6a3b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpAfII2VgpS7"
      },
      "source": [
        "## ライブラリのインストール\n",
        "インストールしていないライブラリがある場合はインストールする\n",
        "\n",
        "polars<br>\n",
        "https://pola-rs.github.io/polars/py-polars/html/reference/dataframe/index.html\n",
        "<br>\n",
        "polarsは毎回最初にインストールが必要っぽい"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fiJQyvFgYhV",
        "outputId": "5229d182-b13c-4ac1-8beb-4de9e7e68004"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.10/dist-packages (0.17.3)\n",
            "Requirement already satisfied: typing_extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from polars) (4.5.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.22.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install polars\n",
        "!pip install scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "699zKTUsg57U"
      },
      "source": [
        "##ライブラリのimport\n",
        "\n",
        "毎回最初に1回実行する。\n",
        "インストールが必要なものがあれば別途!pipでインストールする。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "udnAhkMYI2br"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import polars as pl\n",
        "import random\n",
        "import seaborn as sns\n",
        "import hashlib\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram, distance, fcluster\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF_pXzBcln0Z"
      },
      "source": [
        "# クラスター分析のサンプル"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NJw3sMF4iVO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# 2次元のデータを生成する\n",
        "data = np.random.rand(10, 2)\n",
        "\n",
        "# 距離行列を計算する\n",
        "distance_matrix = linkage(data, method='ward')\n",
        "\n",
        "# リンクの色を指定する関数を定義する\n",
        "def color_func(x):\n",
        "    if x <= 0.4:\n",
        "        return 'C1'\n",
        "    elif x <= 0.8:\n",
        "        return 'C2'\n",
        "    else:\n",
        "        return 'C3'\n",
        "\n",
        "# 樹形図を描画する\n",
        "dendrogram(distance_matrix, link_color_func=lambda x: color_func(x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCSgtGW8kOmn"
      },
      "source": [
        "# 階層型クラスター分析"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "IFHaSqCukH_m"
      },
      "outputs": [],
      "source": [
        "def cluster_analysis(df,\n",
        "                     label_col_name,\n",
        "                     clust_num,\n",
        "                     fig_save_dir,\n",
        "                     rm_col_names,\n",
        "                     plot_fig=False):\n",
        "  len_df = len(df)\n",
        "\n",
        "  # 目的変数を取得して正規化する\n",
        "  X = df.select(pl.all().exclude(label_col_name)).to_numpy()\n",
        "  y = df.select([label_col_name]).to_numpy()\n",
        "  X = StandardScaler().fit_transform(X)\n",
        "\n",
        "  # クラスター分析実行\n",
        "  # 距離行列の計算\n",
        "  dists = distance.pdist(X)\n",
        "\n",
        "  # 階層型クラスタリング(ward法)の実行\n",
        "  Z = linkage(dists, method='ward')\n",
        "  df_dist_vs_cluster_numbers = dist_vs_cluster_numbers(Z, df, fig_save_dir, plot_fig=False)\n",
        "  max_cluster = df_dist_vs_cluster_numbers.get_column('n_cluster').max()\n",
        "\n",
        "  clust_num_r = 0 + clust_num\n",
        "  if clust_num_r > max_cluster:\n",
        "      clust_num_r = 0 + max_cluster\n",
        "\n",
        "  ths = list(map(lambda x: df_dist_vs_cluster_numbers.filter(pl.col('n_cluster') == (clust_num_r + x)).get_column('dist').to_list()[0], [-1, 0]))\n",
        "  clusters = fcluster(Z, t=ths[1], criterion='distance')\n",
        "\n",
        "  clust_nums_list = []\n",
        "  for row in df_dist_vs_cluster_numbers.rows():\n",
        "    clust_nums_list.append(fcluster(Z, t=row[1], criterion='distance'))\n",
        "\n",
        "  clust_nums_df = pl.DataFrame(clust_nums_list)\n",
        "  clust_nums = list(range(1, len(df_dist_vs_cluster_numbers)+1))\n",
        "  clust_nums.reverse()\n",
        "  clust_num_col_names = list(map(lambda x: 'clust_num_' + str(x), clust_nums))\n",
        "  clust_nums_df.columns = clust_num_col_names\n",
        "  clust_nums_df = clust_nums_df.with_columns(df.get_column(label_col_name).alias(label_col_name))\n",
        "  clust_nums_df = clust_nums_df.select([label_col_name] + clust_num_col_names)\n",
        "\n",
        "  # デンドログラムの作成\n",
        "  if len_df < 101:\n",
        "    plt.rcParams['figure.facecolor'] = '#ffffff'\n",
        "    plt.figure(figsize=(15, int(0.3*len(df))))\n",
        "    dn = dendrogram(Z,\n",
        "                    labels=df[label_col_name].to_list(),\n",
        "                    color_threshold=ths[0],\n",
        "                    above_threshold_color='#111111',\n",
        "                    orientation='right')\n",
        "    plt.tick_params(labelsize=12)\n",
        "    plt.xlabel('Distance', fontsize=16)\n",
        "    plt.ylabel(label_col_name, fontsize=16)\n",
        "    plt.savefig(fig_save_dir + 'dendrogram.png')\n",
        "    if plot_fig:\n",
        "        plt.show()\n",
        "  else:\n",
        "      print('Finished without dendrogram (length df is too long (' + str(len_df) + ')')\n",
        "\n",
        "  return [df_dist_vs_cluster_numbers, clust_nums_df, clust_num_r]\n",
        "\n",
        "\n",
        "def dist_vs_cluster_numbers(cluster_analysis_result_df, df, fig_save_dir, plot_fig=False):\n",
        "    n_clusters = len(df)\n",
        "    n_samples = len(df)\n",
        "    df1 = pl.DataFrame(cluster_analysis_result_df)\n",
        "    dists = []\n",
        "    cluster_nums = []\n",
        "\n",
        "    for row in df1.rows():\n",
        "        n_clusters -= 1\n",
        "        dists.append(row[2])\n",
        "        cluster_nums.append(n_clusters)\n",
        "    df2 = pl.DataFrame({'n_cluster': cluster_nums, 'dist': dists})\n",
        "\n",
        "    # 距離 vs クラスター数のグラフを描画\n",
        "    plt.plot(dists, cluster_nums, 'yo-')\n",
        "    plt.title('Threshold dependency of hierarchical clustering')\n",
        "    plt.xlabel('Distance')\n",
        "    plt.ylabel('Num of clusters')\n",
        "    plt.savefig(fig_save_dir + 'distances.png')\n",
        "    if plot_fig:\n",
        "        plt.show()\n",
        "\n",
        "    return df2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXzz5cRBlyrR"
      },
      "source": [
        "# group_name毎にデータフレームを作成し、繰り返しクラスター分析を行うため関数を定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "QxEeAFm1l8Zi"
      },
      "outputs": [],
      "source": [
        "# クラスター分析に用いるパラメータの順列を求めるための関数（ランク付けのソート順に用いる）\n",
        "def fetch_params_parmutations_and_titles(param_arr, desc=False):\n",
        "    param_arr.sort()\n",
        "    arr_num = list(range(0,len(param_arr)))\n",
        "    arr_permu_num = list(itertools.permutations(arr_num))\n",
        "    param_arr_permu = list(map(lambda x: list(map(lambda i: param_arr[i], x)), arr_permu_num))\n",
        "    permu_num_titles = list(map(lambda x: '_'.join(list(map(str, x))), arr_permu_num))\n",
        "    dict_param_permu = dict(zip(permu_num_titles,param_arr_permu))\n",
        "    if desc:\n",
        "        dict_param_permu = dict(list(reversed(dict_param_permu.items())))\n",
        "    else:\n",
        "        dict_param_permu = dict(sorted(dict_param_permu.items()))\n",
        "    return dict_param_permu\n",
        "\n",
        "\n",
        "def fetch_fct_sets_with_permutations_and_titles(fct_sets, desc=False):\n",
        "    dict_res = {}\n",
        "    for fcts in fct_sets:\n",
        "        dict_res['__'.join(fcts)] = fetch_params_parmutations_and_titles(fcts)\n",
        "    print(dict_res)\n",
        "    return dict_res\n",
        "\n",
        "\n",
        "# データフレームを指定したカラムでユニークにする関数\n",
        "def fetch_unique_values_of_df_column(df, column_name):\n",
        "   return df.unique(subset=[column_name]).sort(column_name).get_column(column_name).to_list()\n",
        "\n",
        "\n",
        "# クラスター分析に用いるfctカラムの数値を用いてタイトル(name)を生成する\n",
        "def fetch_names_by_fcts(df,\n",
        "                        fct_columns,\n",
        "                        name_formats):\n",
        "    res = df.select(fct_columns).apply(lambda x: name_formats[0] % x[0] + '_' + name_formats[1] % x[1])\n",
        "    return res\n",
        "\n",
        "\n",
        "def add_name_by_fcts_to_df(df, fct_columns, name_formats):\n",
        "    names = fetch_names_by_fcts(df,\n",
        "                                fct_columns,\n",
        "                                name_formats)\n",
        "    df = df.with_columns(names).rename({\"apply\": \"name\"})\n",
        "    return df\n",
        "\n",
        "\n",
        "# 全てのgroup_nameでのクラスター分析結果を連結してcsvとして保存するための関数\n",
        "def union_all_group_name_cluster_results(pdir_fct_set,\n",
        "                                         cluster_num,\n",
        "                                         group_name):\n",
        "    print('union_all_group_name_cluster_results')\n",
        "    df = pl.read_csv(pdir_fct_set + 'fcts.csv')\n",
        "    group_names = fetch_unique_values_of_df_column(df, group_name)\n",
        "    df_clust_nums_all = pl.DataFrame({})\n",
        "\n",
        "    for g_name in group_names:\n",
        "        pdir_g_name = pdir_fct_set + g_name + '/'\n",
        "        if os.path.exists(pdir_g_name):\n",
        "            flag = True\n",
        "            for i in list(reversed(list(range(2, cluster_num + 1)))):\n",
        "                if flag:\n",
        "                    file_dir = pdir_g_name + 'stats_by_cluster_nums_' + str(i) + '.csv'\n",
        "                    if os.path.exists(file_dir):\n",
        "                        print('file_dir: ' + file_dir)\n",
        "                        sub_df = pl.read_csv(file_dir)\n",
        "                        df_clust_nums_all = pl.concat([df_clust_nums_all, sub_df])\n",
        "                        flag = False\n",
        "        else:\n",
        "            print(pdir_g_name + 'is not exist.')\n",
        "    df_result = df.join(df_clust_nums_all, on=[group_name, 'name'])\n",
        "\n",
        "    return df_result\n",
        "\n",
        "\n",
        "# fct_setsすべての結果をマージするための関数を定義\n",
        "def union_all_fct_set_cluster_analysis_results(pdir,\n",
        "                                               file_name,\n",
        "                                               fct_sets,\n",
        "                                               cluster_num):\n",
        "    pdir_file_name = pdir + file_name + '/'\n",
        "    df_all = pl.DataFrame({})\n",
        "\n",
        "    for fct_columns in fct_sets:\n",
        "        print(fct_columns)\n",
        "        fct_columns.sort()\n",
        "        fct_columns_title = '__'.join(fct_columns)\n",
        "        pdir_fct_set = pdir_file_name + fct_columns_title + '/'\n",
        "        file_dir = pdir_fct_set + '/stats_by_cluster_nums_' + str(cluster_num) + '.csv'       \n",
        "        sub_df = pl.read_csv(file_dir)\n",
        "        sub_df = sub_df.drop(fct_columns)\n",
        "        sub_df = sub_df.with_columns(pl.Series(name=\"fct_columns_title\", values=[fct_columns_title] * len(sub_df)))\n",
        "        df_all = pl.concat([df_all, sub_df])\n",
        "    \n",
        "    return df_all\n",
        "\n",
        "\n",
        "# fct_setsとgroup_namesでクラスター分析をバッチ処理\n",
        "def cluster_analysis_batch_with_fct_sets_and_group_names(pdir,\n",
        "                                                         file_name,\n",
        "                                                         fct_sets,\n",
        "                                                         dict_fcts,\n",
        "                                                         group_name,\n",
        "                                                         cluster_num,\n",
        "                                                         drop_dim_columns,\n",
        "                                                         plot_fig=False):\n",
        "    pdir_file_name = pdir + file_name + '/'\n",
        "\n",
        "    for fct_columns in fct_sets:\n",
        "        fct_columns.sort()\n",
        "        fct_columns_title = '__'.join(fct_columns)\n",
        "        pdir_fct_set = pdir_file_name + fct_columns_title + '/'\n",
        "\n",
        "        df = pl.read_csv(pdir + file_name + '.csv')\n",
        "        dict_fct_sets = fetch_fct_sets_with_permutations_and_titles(fct_sets)\n",
        "\n",
        "        sort_desc_bools = list(map(lambda x: dict_fcts[x]['desc'], fct_columns))\n",
        "        name_formats = list(map(lambda x: dict_fcts[x]['format'], fct_columns))\n",
        "\n",
        "\n",
        "        os.makedirs(pdir_file_name, exist_ok=True)\n",
        "        os.makedirs(pdir_fct_set, exist_ok=True)\n",
        "\n",
        "        df = df.select([group_name] + fct_columns)\n",
        "        df = add_name_by_fcts_to_df(df, fct_columns, name_formats)\n",
        "        df.write_csv(pdir_fct_set + 'fcts.csv')\n",
        "\n",
        "        columns_for_analysis = [group_name, 'name'] + fct_columns\n",
        "        unique_df = df.unique(subset=columns_for_analysis).select(columns_for_analysis)\n",
        "        unique_df = unique_df.sort(*([group_name] + fct_columns), descending=[False] + sort_desc_bools)\n",
        "\n",
        "        df_clust_nums_all = pl.DataFrame({})\n",
        "        dict_param_permu = fetch_params_parmutations_and_titles(fct_columns, desc=True)\n",
        "\n",
        "        # https://pola-rs.github.io/polars/py-polars/html/reference/dataframe/api/polars.dataframe.groupby.GroupBy.__iter__.html\n",
        "        for g_name, sub_df in unique_df.select([group_name, 'name'] + fct_columns).groupby(group_name):\n",
        "            pdir_g_name = pdir_fct_set + g_name + '/'\n",
        "            if os.path.exists(pdir_g_name + 'cluster_nums.csv'):\n",
        "                print('skip ' + g_name)\n",
        "            else:\n",
        "                print(g_name)\n",
        "                sub_df = sub_df.drop(group_name)\n",
        "                print(len(sub_df))\n",
        "                if len(sub_df) < cluster_num - 1:\n",
        "                    print('skip cluster analysis len(sub_df) = ' + str(len(sub_df)) + ' is smaller than cluster_num of ' + str(cluster_num))\n",
        "                else:\n",
        "                    os.makedirs(pdir_g_name, exist_ok=True)\n",
        "                    fig_save_dir = pdir_g_name + '/images/'\n",
        "                    os.makedirs(fig_save_dir, exist_ok=True)\n",
        "                    df_dist_vs_cluster_numbers, clust_nums_df, clust_num_r = cluster_analysis(sub_df,\n",
        "                                                                                              'name',\n",
        "                                                                                              cluster_num,\n",
        "                                                                                              fig_save_dir,\n",
        "                                                                                              drop_dim_columns,\n",
        "                                                                                              plot_fig)\n",
        "                    sub_df_with_clust_nums = sub_df.join(clust_nums_df.select(['name', 'clust_num_' + str(clust_num_r)]), on='name')\n",
        "                    cluster_nums = list(range(1, clust_num_r + 1))\n",
        "                    dict_stats = {'cluster_num': cluster_nums}\n",
        "                    stat_names = []\n",
        "\n",
        "                    for i_fct_column, col_name in enumerate(fct_columns):\n",
        "                        res_describe = list(map(lambda i: sub_df_with_clust_nums.filter(pl.col(\"clust_num_\" + str(clust_num_r))== i).get_column(col_name).describe(), cluster_nums))\n",
        "                        if len(stat_names) < 1:\n",
        "                            stat_names = stat_names + list(res_describe[0].get_column('statistic'))\n",
        "                        for j, stat_name in enumerate(stat_names):\n",
        "                            dict_stats['fct' + str(i_fct_column) + '__' + stat_name] = list(map(lambda x: list(x.get_column('value'))[j], res_describe))\n",
        "\n",
        "                    df_stats = pl.DataFrame(dict_stats)\n",
        "                    for param_title, fct_params in dict_param_permu.items():\n",
        "                        df_stats = df_stats.sort(*list(map(lambda x: 'fct' + str(fct_params.index(x)) + '__mean', fct_params)), descending=[True]*len(fct_params))\n",
        "                        df_cluster_nums_r = pl.DataFrame({'cluster_num': list(df_stats.get_column('cluster_num')), 'rank_by_cluster_' + param_title: cluster_nums})\n",
        "                        df_stats = df_cluster_nums_r.join(df_stats, on='cluster_num')\n",
        "                    \n",
        "                    stat_columns = list(map(lambda w: 'rank_by_cluster_' + w, sorted(dict_param_permu.keys()))) + ['cluster_num', 'fct0__count'] + list(itertools.chain.from_iterable(list(map(lambda x: ['fct' + str(x) + '__' + 'mean', 'fct' + str(x) + '__' + 'std'], list(range(len(fct_columns)))))))\n",
        "                    sub_df_stats = df_stats.select(stat_columns)\n",
        "\n",
        "                    sub_df_clust_nums = clust_nums_df.select(['name', 'clust_num_' + str(clust_num_r)]).with_columns(pl.col('clust_num_' + str(clust_num_r)).cast(pl.Int64))\n",
        "                    sub_df_clust_nums = sub_df_clust_nums.rename({'clust_num_' + str(clust_num_r): 'cluster_num'})\n",
        "\n",
        "                    list_gnames = [g_name] * len(sub_df_clust_nums)\n",
        "                    dict_gnames = {}\n",
        "                    dict_gnames[group_name] = list_gnames\n",
        "                    dict_gnames['name'] = list(sub_df_clust_nums.get_column('name'))\n",
        "                    df_gname = pl.DataFrame(dict_gnames)\n",
        "                    sub_df_clust_nums = df_gname.join(sub_df_clust_nums, on='name')\n",
        "                    sub_df_stats = sub_df_clust_nums.join(sub_df_stats, on='cluster_num')\n",
        "                    sub_df_stats = sub_df_stats.rename({'fct0__count': 'n_of_stat'})\n",
        "                    # df_clust_nums_all = pl.concat([df_clust_nums_all, sub_df_stats])\n",
        "\n",
        "                    # group_name毎の結果を保存\n",
        "                    df_dist_vs_cluster_numbers.write_csv(pdir_g_name + 'dist_vs_cluster_nums.csv')\n",
        "                    clust_nums_df.write_csv(pdir_g_name + 'cluster_nums.csv')\n",
        "                    df_stats.write_csv(pdir_g_name + 'stats_all_by_cluster_nums_' + str(clust_num_r) + '.csv')\n",
        "                    sub_df_stats.write_csv(pdir_g_name + 'stats_by_cluster_nums_' + str(clust_num_r) + '.csv')\n",
        "\n",
        "        df_result = union_all_group_name_cluster_results(pdir_fct_set,\n",
        "                                                         cluster_num,\n",
        "                                                         group_name)\n",
        "        # クラスター分析結果を保存\n",
        "        save_file_name = pdir_fct_set + '/stats_by_cluster_nums_' + str(cluster_num) + '.csv'\n",
        "        print(save_file_name)\n",
        "        df_result.write_csv(save_file_name)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W0JI03Sg7Ky"
      },
      "source": [
        "#主成分分析を行う関数を定義\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "flsoiHNjoSe8"
      },
      "outputs": [],
      "source": [
        "def pca_analysis(df, label_col_name):\n",
        "    n_pca = len(df.columns) - 1\n",
        "    pcs = list(range(1, n_pca + 1))\n",
        "    pc_names = list(map(lambda x: 'pc' + str(x), pcs))\n",
        "\n",
        "    print(len(df))\n",
        "    print(len(df.columns))\n",
        "\n",
        "    # 目的変数を取得して正規化する\n",
        "    X = df.select(pl.all().exclude(label_col_name)).to_numpy()\n",
        "    y = df.select([label_col_name]).to_numpy()\n",
        "    X = StandardScaler().fit_transform(X)\n",
        "\n",
        "    # 主成分分析を実行する\n",
        "    pca = PCA(n_components=n_pca)\n",
        "    principalComponents = pca.fit_transform(X, y)\n",
        "\n",
        "    # 主成分分析の結果をデータフレームに変換する\n",
        "    df_principal = pl.DataFrame(principalComponents, schema = pc_names)\n",
        "    df_pcs = df_principal.with_columns(df.get_column(label_col_name).alias(label_col_name))\n",
        "    col_names = [label_col_name] + pc_names\n",
        "    df_pcs = df_pcs.select(col_names)\n",
        "\n",
        "    # 因子負荷量を取得する\n",
        "    df_loadings = pl.DataFrame(pca.components_.T, schema=pc_names)\n",
        "    features = df.select(pl.all().exclude([label_col_name])).columns\n",
        "    df_loadings = df_loadings.with_columns(pl.Series(features).alias('feature'))\n",
        "    df_loadings = df_loadings.select(['feature'] + pc_names)\n",
        "\n",
        "    # 各成分の寄与率を取得する\n",
        "    var_ratio = pca.explained_variance_ratio_\n",
        "    cumulative_var_ratio = [sum(var_ratio[:i+1]) for i in range(len(var_ratio))]\n",
        "    df_variance_ratio = pl.DataFrame({'pc': pcs,\n",
        "                                    'var_ratio': var_ratio,\n",
        "                                    'cumulative_var_ratio': cumulative_var_ratio})\n",
        "\n",
        "    return [df_variance_ratio, df_pcs, df_loadings]\n",
        "\n",
        "\n",
        "# 主成分分析結果をグラフにする\n",
        "def plot_pca_results(label_col_name,\n",
        "                     fig_save_dir,\n",
        "                     df_variance_ratio,\n",
        "                     df_pcs,\n",
        "                     df_loadings,\n",
        "                     list_pc_score_colors,\n",
        "                     plot_fig=False):\n",
        "    n_pca = len(df_pcs.columns) - 1\n",
        "    pcs = list(range(1, n_pca + 1))\n",
        "    pc_names = list(map(lambda x: 'pc' + str(x), pcs))\n",
        "    plt.rcParams['figure.facecolor'] = '#ffffff'\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.plot(df_variance_ratio.select(['pc']).to_numpy(),\n",
        "            df_variance_ratio.select(['var_ratio']).to_numpy())\n",
        "    ax.plot(df_variance_ratio.select(['pc']).to_numpy(),\n",
        "            df_variance_ratio.select(['cumulative_var_ratio']).to_numpy())\n",
        "    ax.set_xticks(pcs)\n",
        "    ax.set_yticks([0,0.2,0.4,0.6,0.8,1])\n",
        "    plt.xlim(0.8, pcs[-1] + 0.2)\n",
        "    plt.ylim(0, 1)\n",
        "    plt.xlabel('PC')\n",
        "    plt.ylabel('Variance Ratio')\n",
        "\n",
        "    if not os.path.exists(fig_save_dir):\n",
        "        os.makedirs(fig_save_dir, exist_ok=True)\n",
        "\n",
        "    plt.savefig(fig_save_dir + 'var_ratio.png')\n",
        "    if plot_fig:\n",
        "        plt.plot()\n",
        "\n",
        "    max_abs = df_pcs['pc1'].abs().max()*1.05\n",
        "\n",
        "    pc_plot_colors = []\n",
        "    if len(list_pc_score_colors) == 0:\n",
        "        if isinstance(df_pcs[label_col_name][0], int):\n",
        "            color_list = ['red', 'blue', 'green', 'purple', 'orange', 'pink', 'brown', 'yellow']\n",
        "            pc_plot_colors = list(df_pcs[label_col_name].apply(lambda x: color_list[x]))\n",
        "\n",
        "    pc_combinations = list(itertools.combinations(pcs, 2))\n",
        "\n",
        "    for i, j in pc_combinations:\n",
        "        # 主成分プロット\n",
        "        fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n",
        "        if len(list_pc_score_colors) > 0:\n",
        "            list_pc_score_colors_r = list(map(lambda x: 'C' + str(x), list_pc_score_colors))\n",
        "            ax1.scatter(x=df_pcs.select(pc_names[i-1]).to_numpy(),\n",
        "                        y=df_pcs.select([pc_names[j-1]]).to_numpy(),\n",
        "                        c=list_pc_score_colors_r)\n",
        "        elif len(pc_plot_colors) > 0:\n",
        "            ax1.scatter(x=df_pcs.select(pc_names[i-1]).to_numpy(),\n",
        "                        y=df_pcs.select([pc_names[j-1]]).to_numpy(),\n",
        "                        color=pc_plot_colors)\n",
        "        else:\n",
        "            colors = np.linspace(0, 1, len(df_pcs))\n",
        "            ax1.scatter(x=df_pcs.select(pc_names[i-1]).to_numpy(),\n",
        "                        y=df_pcs.select([pc_names[j-1]]).to_numpy(),\n",
        "                        c=colors,\n",
        "                        cmap='coolwarm')\n",
        "        ax1.axhline(0, color='gray', linewidth=0.5)\n",
        "        ax1.axvline(0, color='gray', linewidth=0.5)\n",
        "        pc_score_scale = [x * max_abs * 1.05 for x in [-1,1]]\n",
        "        ax1.set_xlim(pc_score_scale)\n",
        "        ax1.set_ylim(pc_score_scale)\n",
        "\n",
        "        xy_labels = [f\"PC{x} ({df_variance_ratio.get_column('var_ratio')[x-1]*100:.1f}%)\" for x in [i, j]]\n",
        "        ax1.set_xlabel(xy_labels[0])\n",
        "        ax1.set_ylabel(xy_labels[1])\n",
        "\n",
        "        # 因子負荷量プロット\n",
        "        marker = '+'\n",
        "        marker_size = 5\n",
        "        marker_color = 'gray'\n",
        "\n",
        "        x_coors = df_loadings.select(pc_names[i-1]).to_numpy()\n",
        "        y_coors = df_loadings.select(pc_names[j-1]).to_numpy()\n",
        "\n",
        "        ax2.scatter(x=x_coors, y=y_coors, marker=marker, s=marker_size, c=marker_color)\n",
        "        for k in list(range(0, len(x_coors))):\n",
        "            ax2.annotate(str(k+1), (x_coors[k], y_coors[k]), ha='center', va='center')\n",
        "\n",
        "        ax2.axhline(0, color='gray', linewidth=0.5)\n",
        "        ax2.axvline(0, color='gray', linewidth=0.5)\n",
        "        ticks = [-1, -0.5, 0, 0.5, 1]\n",
        "        ax2.set_xticks(ticks)\n",
        "        ax2.set_yticks(ticks)\n",
        "        loading_scale = [-1.1, 1.1]\n",
        "        ax2.set_xlim(loading_scale)\n",
        "        ax2.set_ylim(loading_scale)\n",
        "        ax2.set_xlabel(xy_labels[0])\n",
        "        ax2.set_ylabel(xy_labels[1])\n",
        "\n",
        "        plt.savefig(fig_save_dir + 'pcs_and_loadings_pc' + '_'.join([str(i), str(j)]) + '.png')\n",
        "        if plot_fig:\n",
        "            plt.show()\n",
        "\n",
        "# 以下バッチ処理用の関数\n",
        "# 全てのgroup_nameでの主成分分析結果を連結してcsvとして保存するための関数\n",
        "def union_all_group_name_pca_results(pdir_fct_set,\n",
        "                                     group_name):\n",
        "    print('union_all_group_name_pca_results')\n",
        "    df = pl.read_csv(pdir_fct_set + 'fcts.csv')\n",
        "    group_names = fetch_unique_values_of_df_column(df, group_name)\n",
        "    pca_params = ['pcs', 'loadings', 'var_ratio']\n",
        "\n",
        "    for pca_param in pca_params:\n",
        "        df_all = pl.DataFrame({})\n",
        "        for g_name in group_names:\n",
        "            pdir_g_name = pdir_fct_set + g_name + '/'\n",
        "            if os.path.exists(pdir_g_name):\n",
        "                sub_df = pl.read_csv(pdir_g_name + pca_param + '.csv')\n",
        "                col_names = sub_df.columns\n",
        "                sub_df = sub_df.with_columns(pl.Series(name=group_name, values=[g_name] * len(sub_df)))\n",
        "                sub_df = sub_df.select([group_name] + col_names)\n",
        "                df_all = pl.concat([df_all, sub_df])\n",
        "            else:\n",
        "                print(pdir_g_name + 'is not exist.')\n",
        "\n",
        "        df_all.write_csv(pdir_fct_set + pca_param + '.csv')\n",
        "\n",
        "\n",
        "# fct_setsすべての主成分結果をマージするための関数を定義\n",
        "def union_all_fct_set_pca_results(pdir,\n",
        "                                  file_name,\n",
        "                                  fct_sets):\n",
        "    pdir_file_name = pdir + file_name + '/'\n",
        "    df_alls = {}\n",
        "    pca_params = ['pcs', 'loadings', 'var_ratio']\n",
        "    for pca_param in pca_params:\n",
        "        df_all = pl.DataFrame({})\n",
        "\n",
        "        for fct_columns in fct_sets:\n",
        "            fct_columns.sort()\n",
        "            fct_columns_title = '__'.join(fct_columns)\n",
        "            pdir_fct_set = pdir_file_name + fct_columns_title + '/'\n",
        "            file_dir = pdir_fct_set + pca_param + '.csv'\n",
        "            sub_df = pl.read_csv(file_dir)\n",
        "            col_names = sub_df.columns\n",
        "            sub_df = sub_df.with_columns(pl.Series(name=\"fct_columns_title\", values=[fct_columns_title] * len(sub_df)))\n",
        "            sub_df = sub_df.select(['fct_columns_title'] + col_names)\n",
        "            df_all = pl.concat([df_all, sub_df])\n",
        "        \n",
        "        df_alls[pca_param] = df_all\n",
        "    \n",
        "    return df_alls\n",
        "\n",
        "\n",
        "# fct_setsとgroup_namesで主成分分析をバッチ処理\n",
        "def pca_batch_with_fct_sets_and_group_names(pdir,\n",
        "                                            file_name,\n",
        "                                            fct_sets,\n",
        "                                            dict_fcts,\n",
        "                                            group_name):\n",
        "    pdir_file_name = pdir + file_name + '/'\n",
        "\n",
        "    for fct_columns in fct_sets:\n",
        "        fct_columns.sort()\n",
        "        fct_columns_title = '__'.join(fct_columns)\n",
        "        pdir_fct_set = pdir_file_name + fct_columns_title + '/'\n",
        "        sort_desc_bools = list(map(lambda x: dict_fcts[x]['desc'], fct_columns))\n",
        "        name_formats = list(map(lambda x: dict_fcts[x]['format'], fct_columns))\n",
        "        fcts_file_name = pdir_fct_set + 'fcts.csv'\n",
        "\n",
        "        if os.path.exists(fcts_file_name):\n",
        "            df = pl.read_csv(fcts_file_name)\n",
        "        else:\n",
        "            df = pl.read_csv(pdir + file_name + '.csv')\n",
        "            os.makedirs(pdir_file_name, exist_ok=True)\n",
        "            os.makedirs(pdir_fct_set, exist_ok=True)\n",
        "            df = df.select([group_name] + fct_columns)\n",
        "            df = add_name_by_fcts_to_df(df, fct_columns, name_formats)\n",
        "            df.write_csv(fcts_file_name)\n",
        "        print(df)\n",
        "\n",
        "        columns_for_analysis = [group_name, 'name'] + fct_columns\n",
        "        unique_df = df.unique(subset=columns_for_analysis).select(columns_for_analysis)\n",
        "        unique_df = unique_df.sort(*([group_name] + fct_columns), descending=[False] + sort_desc_bools)\n",
        "\n",
        "        df_clust_nums_all = pl.DataFrame({})\n",
        "        th_pca = 2\n",
        "\n",
        "        # https://pola-rs.github.io/polars/py-polars/html/reference/dataframe/api/polars.dataframe.groupby.GroupBy.__iter__.html\n",
        "        for g_name, sub_df in unique_df.select([group_name, 'name'] + fct_columns).groupby(group_name):\n",
        "            pdir_g_name = pdir_fct_set + g_name + '/'\n",
        "            if len(sub_df) > th_pca:\n",
        "                if os.path.exists(pdir_g_name + 'pcs.csv'):\n",
        "                    print('skip ' + g_name)\n",
        "                else:\n",
        "                    sub_df = sub_df.drop(group_name)\n",
        "                    print(g_name)\n",
        "                    print(len(sub_df))\n",
        "                    if len(sub_df) < th_pca:\n",
        "                        print('skip PCA analysis len(sub_df) < ' + str(th_pca))\n",
        "                    else:\n",
        "                        print(g_name)\n",
        "                        os.makedirs(pdir_g_name, exist_ok=True)\n",
        "                        df_variance_ratio, df_pcs, df_loadings = pca_analysis(sub_df, label_col_name)\n",
        "\n",
        "                        # group_name毎の結果を保存\n",
        "                        df_pcs.write_csv(pdir_g_name + 'pcs.csv')\n",
        "                        df_loadings.write_csv(pdir_g_name + 'loadings.csv')\n",
        "                        df_variance_ratio.write_csv(pdir_g_name + 'var_ratio.csv')\n",
        "            else:\n",
        "                print('skip PCA analysis len(sub_df) < ' + str(th_pca))\n",
        "\n",
        "        union_all_group_name_pca_results(pdir_fct_set, group_name)\n",
        "\n",
        "\n",
        "# 主成分の結果をバッチで図にする\n",
        "def plot_pca_results_batch_with_fct_sets_and_group_names(pdir,\n",
        "                                                         file_name,\n",
        "                                                         group_name,\n",
        "                                                         plot_fig=False):\n",
        "    pdir_file_name = pdir + file_name + '/'\n",
        "    param_names = ['var_ratio', 'pcs', 'loadings', 'clusters']\n",
        "    df_results = {}\n",
        "\n",
        "    for param_name in param_names:\n",
        "        file_dir = pdir_file_name + param_name + '.csv'\n",
        "        if os.path.exists(file_dir):\n",
        "            df = pl.read_csv(file_dir)\n",
        "            if param_name == 'clusters':\n",
        "                df = df.select(['fct_columns_title', group_name, 'name', 'rank_by_cluster_0_1']).unique()\n",
        "            df_results[param_name] = df\n",
        "\n",
        "    df_fct_set_group_names = df_results['pcs'].select(['fct_columns_title', group_name]).unique().sort(['fct_columns_title', group_name])\n",
        "    df_params = {'index': list(range(len(df_fct_set_group_names)))}\n",
        "    df_params[group_name] = df_fct_set_group_names.get_column(group_name).to_list()\n",
        "    df_params['fct_columns_title'] = df_fct_set_group_names.get_column('fct_columns_title').to_list()\n",
        "    \n",
        "    for i in df_params['index']:\n",
        "        df_filter = {}\n",
        "        for col_name in ['fct_columns_title', group_name]:\n",
        "            df_filter[col_name] = df_params[col_name][i]\n",
        "        \n",
        "        sub_df_results = {}\n",
        "        for param_name in param_names:\n",
        "            sub_df = df_results[param_name]\n",
        "            for filter_name, filter_value in df_filter.items():\n",
        "                sub_df = sub_df.filter(pl.col(filter_name) == filter_value)\n",
        "            sub_df = sub_df.drop(['fct_columns_title', group_name])            \n",
        "            sub_df_results[param_name] = sub_df\n",
        "        \n",
        "        pcs = sub_df_results['pcs'].join(sub_df_results['clusters'].select(['name', 'rank_by_cluster_0_1']), on=['name']).sort('rank_by_cluster_0_1')\n",
        "        sub_df_results['pcs'] = pcs.drop('rank_by_cluster_0_1')\n",
        "        clusters = pcs.get_column('rank_by_cluster_0_1').to_list()\n",
        "        sub_df_results['clusters'] = [] + clusters\n",
        "        \n",
        "        pdir_fct_set = pdir_file_name + df_filter['fct_columns_title'] + '/'\n",
        "        pdir_g_name = pdir_fct_set + df_filter[group_name] + '/'\n",
        "        fig_save_dir = pdir_g_name + 'images/'\n",
        "        plot_pca_results('name', fig_save_dir, *sub_df_results.values(), plot_fig)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NbTr9OB8EZu"
      },
      "source": [
        "# パラメータを定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "75DPCZ0hZqMG"
      },
      "outputs": [],
      "source": [
        "cluster_num = 5\n",
        "pdir = '/content/drive/MyDrive/csv/'\n",
        "file_name = 'access__env_daily_20230429_test3'\n",
        "group_name = 'environment_uuid'\n",
        "dict_fcts = {\n",
        "    'ac': {'format': '%.0f', 'desc': True},\n",
        "    'uu': {'format': '%.0f', 'desc': True},\n",
        "    'norm': {'format': '%.1f', 'desc': True},\n",
        "    'angle': {'format': '%.1f', 'desc': True},\n",
        "    'log_ac': {'format': '%.2f', 'desc': True},\n",
        "    'log_uu': {'format': '%.2f', 'desc': True},\n",
        "    'log_norm': {'format': '%.2f', 'desc': True}\n",
        "}\n",
        "fct_sets = [['ac', 'uu'],\n",
        "            ['norm', 'angle'],\n",
        "            ['log_ac', 'log_uu'],\n",
        "            ['log_norm', 'angle']]\n",
        "drop_dim_columns = []\n",
        "plot_fig = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKkWFbtC8K7K"
      },
      "source": [
        "# クラスター分析を実行"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vH71fkWf5jm"
      },
      "outputs": [],
      "source": [
        "cluster_analysis_batch_with_fct_sets_and_group_names(pdir,\n",
        "                                                     file_name,\n",
        "                                                     fct_sets,\n",
        "                                                     dict_fcts,\n",
        "                                                     group_name,\n",
        "                                                     cluster_num,\n",
        "                                                     drop_dim_columns,\n",
        "                                                     plot_fig)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# すべての結果を結合してファイルに保存\n",
        "res_df = union_all_fct_set_cluster_analysis_results(pdir,\n",
        "                                                    file_name,\n",
        "                                                    fct_sets,\n",
        "                                                    cluster_num)\n",
        "print(res_df)\n",
        "res_df.write_csv(pdir + file_name + '/' + 'clusters.csv')"
      ],
      "metadata": {
        "id": "YbyZlEOrqaeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 主成分分析を実行"
      ],
      "metadata": {
        "id": "98Y4SyhHHFml"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QIP_o64DVcK"
      },
      "outputs": [],
      "source": [
        "label_col_name = 'name'\n",
        "pca_batch_with_fct_sets_and_group_names(pdir,\n",
        "                                        file_name,\n",
        "                                        fct_sets,\n",
        "                                        dict_fcts,\n",
        "                                        group_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 結果を結合\n",
        "df_pca_results = union_all_fct_set_pca_results(pdir,\n",
        "                                               file_name,\n",
        "                                               fct_sets)\n",
        "print(df_pca_results)\n",
        "\n",
        "# 保存\n",
        "for key, item in df_pca_results.items():\n",
        "    item.write_csv(pdir + file_name + '/' + key + '.csv')"
      ],
      "metadata": {
        "id": "mNRyFXm4eEu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 主成分分析の図を保存\n",
        "plot_pca_results_batch_with_fct_sets_and_group_names(pdir,\n",
        "                                                     file_name,\n",
        "                                                     group_name,\n",
        "                                                     plot_fig)"
      ],
      "metadata": {
        "id": "c3svHItum7su"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "XF_pXzBcln0Z"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyMXplFuuEwpMV0HCyxjSAXI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}